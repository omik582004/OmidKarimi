{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# **Big Data with Google Colab using Python**\n", "This notebook introduces **Big Data Analysis** in **Google Colab** using Python libraries such as **Dask, Pandas, and Google BigQuery**."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **1. Setting up Google Colab for Big Data**\n", "Google Colab provides a free cloud-based environment with powerful computing resources."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Check system specifications\n", "!cat /proc/cpuinfo | grep \"model name\" | uniq\n", "!cat /proc/meminfo | grep \"MemTotal\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **2. Installing and Using Dask for Parallel Computing**\n", "Dask is a powerful parallel computing library for handling big data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install dask\n", "import dask.dataframe as dd\n", "\n", "# Example: Load a large dataset\n", "url = \"https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv\"\n", "df = dd.read_csv(url)\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **3. Processing Large Datasets with Pandas and Dask**\n", "Let's analyze a large dataset using **Dask** to distribute computations efficiently."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute basic statistics\n", "df.describe().compute()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **4. Data Visualization for Big Data**\n", "Visualizing large datasets efficiently."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "df.compute().hist(figsize=(10, 5))\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **5. Using Google BigQuery for Large-Scale Data Analysis**\n", "Google BigQuery allows querying massive datasets using SQL."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from google.colab import auth\n", "auth.authenticate_user()\n", "\n", "from google.cloud import bigquery\n", "client = bigquery.Client()\n", "\n", "query = \"\"\"\n", "SELECT COUNT(*) AS total_trips, AVG(trip_distance) AS avg_distance\n", "FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n", "WHERE pickup_datetime BETWEEN '2015-01-01' AND '2015-12-31'\n", "\"\"\"\n", "\n", "df_bigquery = client.query(query).to_dataframe()\n", "df_bigquery"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **Conclusion**\n", "This notebook demonstrated how to handle **big data in Google Colab** using:\n", "- **Dask** for parallel computing\n", "- **Pandas** for large dataset processing\n", "- **Matplotlib** for visualization\n", "- **Google BigQuery** for large-scale SQL analysis\n", "\n", "\ud83d\ude80 **Now you can analyze large datasets efficiently in Colab!**"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 4}